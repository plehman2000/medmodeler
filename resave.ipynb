{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cd135d5b-76da-4d25-b7c5-a9d4815c62ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUNNING ON ENVIRONMENT 'UNEXT'\n",
    "# Config\n",
    "seed = 42  # for reproducibility\n",
    "training_split_ratio = 0.9  # use 90% of samples for training, 10% for testing\n",
    "num_epochs = 5\n",
    "# If the following values are False, the models will be downloaded and not computed\n",
    "compute_histograms = False\n",
    "train_whole_images = False \n",
    "train_patches = False\n",
    "import uuid\n",
    "from IPython.display import display\n",
    "import enum\n",
    "import time\n",
    "import random\n",
    "import multiprocessing\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "import torchvision\n",
    "import torchio as tio\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from unet import UNet\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import display\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "plt.rcParams['figure.figsize'] = 12, 6\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "import torch\n",
    "import torchio\n",
    "import pytorch_lightning as pl\n",
    "import wandb\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7445d8-f643-4a93-866e-7943dfab48f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "67da409e-3a70-4097-b2a9-015216239fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "from matplotlib import cm\n",
    "import PIL\n",
    "import torchvision.transforms.functional as Ft\n",
    "import cv2\n",
    "\n",
    "transform = T.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e5d9ae25-3b4a-4185-951f-5d1a84017a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_transforms = tio.Compose([tio.transforms.ToCanonical(),\n",
    "                   tio.RescaleIntensity((0, 1))])\n",
    "augment = tio.Compose([\n",
    "            tio.RandomAffine(),\n",
    "            tio.RandomMotion(p=0.1),\n",
    "            tio.RandomBiasField(p=0.25),\n",
    "        ])\n",
    "def sub_to_aug_pair(subject: tio.Subject):\n",
    "    \n",
    "    transformed_subject = base_transforms(subject)\n",
    "    augmented_subject = augment(transformed_subject)\n",
    "    \n",
    "    return transformed_subject, augmented_subject\n",
    "\n",
    "def tensor_to_disk(path_name, data):\n",
    "    with open(f'{path_name}.npy', 'wb') as f:\n",
    "        np.save(f, data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "960dd764-b774-4855-a4a0-0ae782dffbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVEPATH = os.getcwd() + \"/orange/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8ac93439-b027-4d5e-bed3-06e334eeaae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/patricklehman/MRI/orange/data'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAVEPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9c468f94-c2f0-41d1-bc7e-2e296058556c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_disk(path_name, data):\n",
    "    with open(f'{path_name}.npy', 'wb') as f:\n",
    "        np.save(f, data.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "930c05b2-bfb7-4274-8632-296cdd4225b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mem_to_new_mem():\n",
    "    PATH = \"/home/patricklehman/MRI\"\n",
    "    dataset_dir = Path(PATH +  \"/orange/org/\") #switched from augfix due to one hot error\n",
    "    images_dir = dataset_dir / 'images'\n",
    "    labels_dir = dataset_dir / 'labels'\n",
    "    image_paths = sorted(images_dir.glob('*.nii.gz'))\n",
    "    label_paths = sorted(labels_dir.glob('*.nii.gz'))\n",
    "    assert len(image_paths) == len(label_paths)\n",
    "    dataset = []\n",
    "    count = 0\n",
    "    for (image_path, label_path) in tqdm(zip(image_paths, label_paths), total=len(image_paths)):\n",
    "        if count > 33:\n",
    "            subject = tio.Subject(\n",
    "                sample=tio.ScalarImage(image_path),\n",
    "                label =  tio.LabelMap(label_path))\n",
    "            norm, aug = sub_to_aug_pair(subject)\n",
    "    #         dataset.extend([norm,aug])\n",
    "    #         return dataset\n",
    "            tensor_to_disk(SAVEPATH + f\"/label/label{count}\", norm['label'][tio.DATA])        \n",
    "            tensor_to_disk(SAVEPATH + f\"/sample/sample{count}\", norm['sample'][tio.DATA])        \n",
    "            count +=1\n",
    "            tensor_to_disk(SAVEPATH + f\"/label/label{count}\", aug['label'][tio.DATA])        \n",
    "            tensor_to_disk(SAVEPATH + f\"/sample/sample{count}\", aug['sample'][tio.DATA])        \n",
    "            count +=1\n",
    "\n",
    "    #         dataset.append({\"sample\":, \"label\":norm['label'][tio.DATA]})\n",
    "    #         dataset.append({\"sample\":aug['sample'][tio.DATA], \"label\":aug['label'][tio.DATA]})\n",
    "            del subject#TODO\n",
    "            del aug\n",
    "            del norm\n",
    "        else:\n",
    "            count +=2\n",
    "        \n",
    "        \n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ec1d01ed-8092-456b-a215-8337d1ec20ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 20/139 [03:37<21:32, 10.86s/it]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mem_to_new_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0543a03c-b6c4-4b43-a8b8-a6a1de0a680e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 0.4511, 0.4513,  ..., 0.9957, 0.9966, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "d = torch.tensor(data)\n",
    "print(torch.unique(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9f7cfc3-c765-4467-9399-bc02b632ef33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "30c59f52-5e92-4a71-b25e-1fe509569b06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "44964b1d-fdd2-4be1-bef6-b3e9a8267ab4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/139 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "subject_list = mem_to_sub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "9d08dced-11cf-4529-9fcc-168a23fd506e",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug, n = sub_to_aug_pair(subject_list[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3123c503-ac44-4292-a9dc-2c5c3df18406",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "ce3f1fcf-596e-4645-a3a4-d9ef3c6c14e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "dc04cbac-d401-47a8-8566-c0a9088c40de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#Visualizing tensor functions\n",
    "def write_video(file_path, frames, fps, grayscale=False):\n",
    "    \"\"\"\n",
    "    Writes frames to an mp4 video file\n",
    "    :param file_path: Path to output video, must end with .mp4\n",
    "    :param frames: List of PIL.Image objects\n",
    "    :param fps: Desired frame rate\n",
    "    \"\"\"\n",
    "    w, h = frames[0].size\n",
    "    fourcc = cv2.VideoWriter_fourcc('m', 'p', '4', 'v')\n",
    "    writer = cv2.VideoWriter(file_path, fourcc, fps, (w, h))\n",
    "\n",
    "    for frame in frames:\n",
    "        open_cv_image = np.asarray(frame)\n",
    "        if not grayscale:\n",
    "            # Convert RGB to BGR \n",
    "            open_cv_image = open_cv_image[:, :, ::-1]\n",
    "           \n",
    "        else:\n",
    "            open_cv_image = cv2.cvtColor(open_cv_image, cv2.COLOR_GRAY2BGR)\n",
    "        \n",
    "        writer.write(open_cv_image)\n",
    "\n",
    "    writer.release() \n",
    "\n",
    "def sample_to_video(inputs, name=\"label_vid\", repeat=3):\n",
    "    inputs = inputs[0]\n",
    "    slices = []\n",
    "    for i in range(inputs.size()[2]):\n",
    "        new = inputs[:,:,i]\n",
    "        pil_image = transform(new)\n",
    "        for _ in range(repeat):\n",
    "            slices.append(pil_image)\n",
    "        \n",
    "    return slices\n",
    "\n",
    "\n",
    "def label_to_video(inputs, name=\"label_vid\", repeat=3):\n",
    "    \"\"\"\n",
    "    Given an output tensor from the network with shape [6, 256, 256, 10]), return a\n",
    "    video of the tensor with name \"name\"\n",
    "    \"\"\"\n",
    "    num_classes=6\n",
    "    color = (torch.ones((1,inputs.shape[1],inputs.shape[2],inputs.shape[3]))/num_classes)\n",
    "    re_colored = inputs * color\n",
    "    re_colored = re_colored[0]\n",
    "    slices = []\n",
    "    magma = cm.get_cmap('YlGnBu')\n",
    "    for i in range(re_colored.size()[2]):\n",
    "        new = re_colored[:,:,i]\n",
    "        new = magma(new)\n",
    "        new = new[:,:,:3]*255\n",
    "        pil_image = PIL.Image.fromarray(new.astype(np.uint8))\n",
    "#         display(pil_image)\n",
    "        for _ in range(repeat):\n",
    "            slices.append(pil_image)\n",
    "    return slices\n",
    "\n",
    "slices=label_to_video(tens['label'])\n",
    "write_video(os.getcwd() + \"/m.mp4\", slices, 10)\n",
    "\n",
    "slices=sample_to_video(d[0]['sample'])\n",
    "write_video(os.getcwd() + \"/test_sample_vid.mp4\", slices, 10, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "3b871563-d631-48ef-9a10-fbf696c3be50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0d34d4-ab4b-45be-a45e-5316708fafe2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97da837c-f25b-4152-b9bf-7c0aa3a7af08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f2c4db-e805-4509-8cd2-2e303e6f9b32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e919e09-fb57-4811-bbc7-b665e1d4515c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36eb5c9-0419-4bb3-a1f0-ad639669184e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d4f626-72d0-4c05-84a9-be31f67641f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bad0c9d-563d-4c81-8b79-deb980eaf752",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ffa89a-1a83-46b7-aa5f-406c50bfdfa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a217d6-981b-4528-b0fc-d22717335a3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit ('new')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "5c39067fb85dad44b96a991ff273746e02ffde2a07ab0e300120c2298a074d00"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
