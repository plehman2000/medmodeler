{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 512, 75])\n",
      "torch.Size([1, 512, 512, 5]) torch.Size([1, 6, 512, 512, 5])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import PIL\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "samp = np.load(os.getcwd() + \"/DATA/sample/sample0.npy\")\n",
    "label = np.load(os.getcwd() + \"/DATA/label/label0.npy\")\n",
    "SLICE_START = 0\n",
    "SLICE_LENGTH = 5\n",
    "sample = torch.tensor(samp, dtype=float)\n",
    "print(sample.shape)\n",
    "label = torch.tensor(label, dtype=float)\n",
    "sample = torch.narrow(sample, 3, SLICE_START, SLICE_LENGTH)\n",
    "label = torch.narrow(label, 3, SLICE_START, SLICE_LENGTH)\n",
    "lab0 = torch.where(label==0,1,0);lab1 = torch.where(label==1,1,0)\n",
    "lab2 = torch.where(label==2,1,0);lab3 = torch.where(label==3,1,0)\n",
    "lab4 = torch.where(label==4,1,0);lab5 = torch.where(label==5,1,0)\n",
    "master_label = torch.stack([lab0,lab1,lab2,lab3,lab4,lab5], dim = 1)\n",
    "\n",
    "print(sample.shape, master_label.shape)\n",
    "\n",
    "from matplotlib import cm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "torch.Size([1, 1, 5, 512, 512]) torch.Size([1, 6, 5, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "sample = torch.reshape(sample, ( 1,1,sample.shape[3], sample.shape[1], sample.shape[2]))\n",
    "master_label = torch.reshape(master_label, (1, master_label.shape[1],master_label.shape[4], master_label.shape[2], master_label.shape[3]))\n",
    "print(master_label.shape[1])\n",
    "\n",
    "print( sample.shape, master_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "transform = T.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampshape: torch.Size([1, 5, 512, 512])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32me:\\Projects\\Lab\\MRI\\SAVE_AS_VIDEO.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Projects/Lab/MRI/SAVE_AS_VIDEO.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m sample_slices \u001b[39m=\u001b[39m sample_to_slices(sample, repeat\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m)\n",
      "\u001b[1;32me:\\Projects\\Lab\\MRI\\SAVE_AS_VIDEO.ipynb Cell 4\u001b[0m in \u001b[0;36msample_to_slices\u001b[1;34m(inputs, name, repeat)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Projects/Lab/MRI/SAVE_AS_VIDEO.ipynb#X15sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(inputs\u001b[39m.\u001b[39msize()[\u001b[39m2\u001b[39m]):\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Projects/Lab/MRI/SAVE_AS_VIDEO.ipynb#X15sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     new \u001b[39m=\u001b[39m inputs[:,:,i]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Projects/Lab/MRI/SAVE_AS_VIDEO.ipynb#X15sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     pil_image \u001b[39m=\u001b[39m transform(new)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Projects/Lab/MRI/SAVE_AS_VIDEO.ipynb#X15sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m# display(pil_image)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Projects/Lab/MRI/SAVE_AS_VIDEO.ipynb#X15sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(repeat):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\transforms\\transforms.py:227\u001b[0m, in \u001b[0;36mToPILImage.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, pic):\n\u001b[0;32m    219\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    220\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m    221\u001b[0m \u001b[39m        pic (Tensor or numpy.ndarray): Image to be converted to PIL Image.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    225\u001b[0m \n\u001b[0;32m    226\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 227\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mto_pil_image(pic, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmode)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\transforms\\functional.py:289\u001b[0m, in \u001b[0;36mto_pil_image\u001b[1;34m(pic, mode)\u001b[0m\n\u001b[0;32m    287\u001b[0m     \u001b[39mif\u001b[39;00m pic\u001b[39m.\u001b[39mis_floating_point() \u001b[39mand\u001b[39;00m mode \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mF\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    288\u001b[0m         pic \u001b[39m=\u001b[39m pic\u001b[39m.\u001b[39mmul(\u001b[39m255\u001b[39m)\u001b[39m.\u001b[39mbyte()\n\u001b[1;32m--> 289\u001b[0m     npimg \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mtranspose(pic\u001b[39m.\u001b[39;49mcpu()\u001b[39m.\u001b[39;49mnumpy(), (\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m))\n\u001b[0;32m    291\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(npimg, np\u001b[39m.\u001b[39mndarray):\n\u001b[0;32m    292\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInput pic must be a torch.Tensor or NumPy ndarray, not \u001b[39m\u001b[39m{\u001b[39m\u001b[39mtype(npimg)}\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Numpy is not available"
     ]
    }
   ],
   "source": [
    "sample_slices = sample_to_slices(sample, repeat=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "#expects tensor of shape (1,1,slices,W,H)\n",
    "def sample_to_slices(inputs, name=\"label_vid\", repeat=3):\n",
    "    inputs = inputs[0]\n",
    "    slices = []\n",
    "    print(f\"sampshape: {inputs.shape}\")\n",
    "    for i in range(inputs.size()[2]):\n",
    "        new = inputs[:,:,i]\n",
    "        pil_image = transform(new)\n",
    "        # display(pil_image)\n",
    "        for _ in range(repeat):\n",
    "            slices.append(pil_image)\n",
    "        \n",
    "    return slices\n",
    "\n",
    "def write_video(file_path, frames, fps, grayscale=False):\n",
    "    \"\"\"\n",
    "    Writes frames to an mp4 video file\n",
    "    :param file_path: Path to output video, must end with .mp4\n",
    "    :param frames: List of PIL.Image objects\n",
    "    :param fps: Desired frame rate\n",
    "    \"\"\"\n",
    "    w, h = frames[0].size\n",
    "    fourcc = cv2.VideoWriter_fourcc('m', 'p', '4', 'v')\n",
    "    writer = cv2.VideoWriter(file_path, fourcc, fps, (w, h))\n",
    "\n",
    "    for frame in frames:\n",
    "        # open_cv_image = np.asarray(frame)\n",
    "        # # print(f\"frameshape: {open_cv_image.shape}\")\n",
    "        # # img = Image.fromarray(open_cv_image)\n",
    "        # # img.save('nu.png')\n",
    "        # # break\n",
    "        # if not grayscale:\n",
    "        #     # Convert RGB to BGR \n",
    "        #     open_cv_image = open_cv_image[:, :, ::-1]\n",
    "           \n",
    "        # else:\n",
    "        #     open_cv_image = cv2.cvtColor(open_cv_image, cv2.COLOR_GRAY2BGR)\n",
    "        open_cv_image = cv2.cvtColor(np.array(frame), cv2.COLOR_RGB2BGR)\n",
    "        writer.write(open_cv_image)\n",
    "\n",
    "    writer.release() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAMPLE TO VIDEO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "transform = T.ToPILImage()\n",
    "\n",
    "#expects tensor of shape (1,1,slices,W,H)\n",
    "def sample_to_slices(inputs, name=\"label_vid\", repeat=3):\n",
    "    inputs = inputs[0]\n",
    "    slices = []\n",
    "    print(f\"sampshape: {inputs.shape}\")\n",
    "    for i in range(inputs.size()[2]):\n",
    "        new = inputs[:,:,i]\n",
    "        pil_image = transform(new)\n",
    "        for _ in range(repeat):\n",
    "            slices.append(pil_image)\n",
    "        \n",
    "    return slices\n",
    "\n",
    "def write_video(file_path, frames, fps):\n",
    "    \"\"\"\n",
    "    Writes frames to an mp4 video file\n",
    "    :param file_path: Path to output video, must end with .mp4\n",
    "    :param frames: List of PIL.Image objects\n",
    "    :param fps: Desired frame rate\n",
    "    \"\"\"\n",
    "    w, h = frames[0].size\n",
    "    fourcc = cv2.VideoWriter_fourcc('m', 'p', '4', 'v')\n",
    "    writer = cv2.VideoWriter(file_path, fourcc, fps, (w, h))\n",
    "\n",
    "    for frame in frames:\n",
    "        open_cv_image = cv2.cvtColor(np.array(frame), cv2.COLOR_RGB2BGR)\n",
    "        writer.write(open_cv_image)\n",
    "\n",
    "    writer.release() \n",
    "\n",
    "\n",
    "def sample_to_video(sample, filename, repeat, fps):\n",
    "    slices=sample_to_slices(sample, repeat)\n",
    "    write_video(os.getcwd() + f\"/{filename}.mp4\", slices, fps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_to_video(sample, \"new\", repeat=1, fps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "video_path = \"new.mp4\"\n",
    "reader = torchvision.io.VideoReader(video_path, \"video\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def label_to_video(inputs, name=\"label_vid\", repeat=3):\n",
    "    \"\"\"\n",
    "    Given an output tensor from the network with shape [6, 256, 256, 10]), return a\n",
    "    video of the tensor with name \"name\"\n",
    "    \"\"\"\n",
    "    num_classes=6\n",
    "    color = (torch.ones((1,inputs.shape[1],inputs.shape[2],inputs.shape[3]))/num_classes)\n",
    "    re_colored = inputs * color\n",
    "    re_colored = re_colored[0]\n",
    "    slices = []\n",
    "    magma = cm.get_cmap('YlGnBu')\n",
    "    for i in range(re_colored.size()[2]):\n",
    "        new = re_colored[:,:,i]\n",
    "        new = magma(new)\n",
    "        new = new[:,:,:3]*255\n",
    "        pil_image = PIL.Image.fromarray(new.astype(np.uint8))\n",
    "#         display(pil_image)\n",
    "        \n",
    "    # return slices\n",
    "\n",
    "\n",
    "\n",
    "#sample\n",
    "img_size = (SLICE_LENGTH, 256, 256)\n",
    "\n",
    "\n",
    "# sample = torch.ones(1, 1, img_size[0], img_size[1], img_size[2])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit ('new')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5c39067fb85dad44b96a991ff273746e02ffde2a07ab0e300120c2298a074d00"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
