{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXkCTL7DAQaR"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "b0NdJiFW3Uy7",
        "outputId": "a975549f-6a17-4dda-9faa-23693c3c1ab3"
      },
      "outputs": [],
      "source": [
        "#RUNNING ON ENVIRONMENT 'UNEXT'\n",
        "# Config\n",
        "seed = 42  # for reproducibility\n",
        "training_split_ratio = 0.9  # use 90% of samples for training, 10% for testing\n",
        "num_epochs = 5\n",
        "\n",
        "# If the following values are False, the models will be downloaded and not computed\n",
        "compute_histograms = False\n",
        "train_whole_images = False \n",
        "train_patches = False\n",
        "\n",
        "from IPython.display import display\n",
        "import enum\n",
        "import time\n",
        "import random\n",
        "import multiprocessing\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchio as tio\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "from unet import UNet\n",
        "from scipy import stats\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from IPython.display import display\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "num_workers = multiprocessing.cpu_count()\n",
        "plt.rcParams['figure.figsize'] = 12, 6\n",
        "\n",
        "print('Last run on', time.ctime())\n",
        "print('TorchIO version:', tio.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "dataset_dir = Path(os.getcwd() + \"/DATA/\")\n",
        "images_dir = dataset_dir / 'images'\n",
        "labels_dir = dataset_dir / 'labels'\n",
        "image_paths = sorted(images_dir.glob('*.nii'))\n",
        "label_paths = sorted(labels_dir.glob('*.nii'))\n",
        "print(len(label_paths))\n",
        "print(len(image_paths))\n",
        "\n",
        "assert len(image_paths) == len(label_paths)\n",
        "\n",
        "\n",
        "import time\n",
        "modded_subjects = []\n",
        "\n",
        "for (image_path, label_path) in tqdm((zip(image_paths, label_paths))):\n",
        "    subject = tio.Subject(\n",
        "        sample=tio.ScalarImage(image_path),\n",
        "        label =  tio.LabelMap(label_path)\n",
        "    )\n",
        "    # print(subject['label1'].tensor[:,:,:,0].shape)\n",
        "\n",
        "    modded_subjects.append(subject)\n",
        "    time.sleep(0.5)\n",
        "\n",
        "modded_subjects = modded_subjects*5\n",
        "\n",
        "dataset = tio.SubjectsDataset(modded_subjects*5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset[0].plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mul = 5\n",
        "shapey = (48, mul*60, mul*48)\n",
        "RESAMPLE_SPACE = 1 #RETAINS BASE RESOLUTION\n",
        "training_transform = tio.Compose([\n",
        "    tio.ToCanonical(),\n",
        "    tio.Resample(RESAMPLE_SPACE),\n",
        "    tio.RandomMotion(p=0.2),\n",
        "    tio.RandomBiasField(p=0.3),\n",
        "    tio.ZNormalization(masking_method=tio.ZNormalization.mean),\n",
        "    tio.RandomNoise(p=0.5),\n",
        "    tio.RandomFlip(),\n",
        "    tio.OneOf({\n",
        "        tio.RandomAffine(): 0.8,\n",
        "        tio.RandomElasticDeformation(): 0.2,\n",
        "    }),\n",
        "    tio.CropOrPad(shapey), #should this be after the flip?\n",
        "    tio.RescaleIntensity((0,1)),\n",
        "    tio.OneHot(),\n",
        "])\n",
        "\n",
        "validation_transform = tio.Compose([\n",
        "    tio.ToCanonical(),\n",
        "    tio.Resample(RESAMPLE_SPACE),\n",
        "    tio.CropOrPad(shapey),\n",
        "    tio.ZNormalization(masking_method=tio.ZNormalization.mean),\n",
        "    tio.OneHot(),\n",
        "])\n",
        "\n",
        "\n",
        "dataset = tio.SubjectsDataset(modded_subjects) #can multiply size because of augmentation\n",
        "\n",
        "num_subjects = len(dataset)\n",
        "num_training_subjects = int(training_split_ratio * num_subjects)\n",
        "num_validation_subjects = num_subjects - num_training_subjects\n",
        "\n",
        "num_split_subjects = num_training_subjects, num_validation_subjects\n",
        "print(num_split_subjects)\n",
        "training_subjects, validation_subjects = torch.utils.data.random_split(modded_subjects, num_split_subjects)\n",
        "\n",
        "training_set = tio.SubjectsDataset(\n",
        "    training_subjects, transform=training_transform)\n",
        "\n",
        "validation_set = tio.SubjectsDataset(\n",
        "    validation_subjects, transform=validation_transform)\n",
        "\n",
        "training_set_raw = [] \n",
        "val_set_raw = []\n",
        "\n",
        "print('Training set:', len(training_set), 'subjects')\n",
        "print('Validation set:', len(validation_set), 'subjects')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Lightning Converted Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pytorch_lightning as pl\n",
        "import os\n",
        "import time\n",
        "DATA_PATH = Path(os.getcwd() + \"/DATA/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "#non class functions\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "  \n",
        "import torchvision.transforms.functional as Ft\n",
        "\n",
        "def prepare_batch(batch):\n",
        "    inputs = batch['sample'][tio.DATA]\n",
        "    targets = batch['label'][tio.DATA]\n",
        "    return inputs, targets\n",
        "def get_dice_score(output, target, epsilon=1e-9):\n",
        "    # print(f\"Output: {type(output)}, {output.shape}\")\n",
        "    # print(f\"Target: {type(target)}, {output.shape}\")\n",
        "    p0 = output\n",
        "    g0 = target\n",
        "    p1 = 1 - p0\n",
        "    g1 = 1 - g0\n",
        "    tp = (p0 * g0).sum(dim=SPATIAL_DIMENSIONS)\n",
        "    fp = (p0 * g1).sum(dim=SPATIAL_DIMENSIONS)\n",
        "    fn = (p1 * g0).sum(dim=SPATIAL_DIMENSIONS)\n",
        "    num = 2 * tp\n",
        "    denom = 2 * tp + fp + fn + epsilon\n",
        "    dice_score = num / denom\n",
        "    return dice_score\n",
        "\n",
        "def get_dice_loss(output, target):\n",
        "    loss = 1 - get_dice_score(output, target)\n",
        "    if type(loss) is None:\n",
        "        return -1\n",
        "    # print(f\"LOSS TYPE: {type(loss)}\")\n",
        "    return loss\n",
        "\n",
        "\n",
        "\n",
        "def tensor_to_video(inputs, name=\"TEST\"):\n",
        "  slices = []\n",
        "  print()\n",
        "  for i in range(inputs.size()[2]):\n",
        "      slice = inputs[0, :,i,:,:];img = Ft.to_pil_image(slice, mode='L')\n",
        "      # print(slice.size())\n",
        "      slices.append(img)\n",
        "  fourcc = cv2.VideoWriter_fourcc(*'mp4v') \n",
        "  video = cv2.VideoWriter(f'{name}.mp4', fourcc, inputs.size()[2], (inputs.size()[4]  , inputs.size()[3]))\n",
        "\n",
        "  for j in slices:\n",
        "    j = np.array(j)\n",
        "    shape = np.shape(j)\n",
        "    opencvImage = cv2.cvtColor(j, cv2.COLOR_GRAY2BGR)\n",
        "    video.write(opencvImage)\n",
        "\n",
        "  cv2.destroyAllWindows()\n",
        "  video.release()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "RESAMPLE = 1 #4\n",
        "class CATDataModule(pl.LightningDataModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        image_mul = 4\n",
        "        self.image_shape = (48, image_mul*60, image_mul*48)\n",
        "        self.training_subjects = []\n",
        "        self.validation_subjects = []\n",
        "        self.train_batch_size = 1\n",
        "        self.val_batch_size = 1\n",
        "        self.dataset = []\n",
        "        self.num_workers = 8\n",
        "\n",
        "    def prepare_data(self, train_batch_size, val_batch_size):\n",
        "        dataset_dir = DATA_PATH\n",
        "        images_dir = dataset_dir / 'images'\n",
        "        labels_dir = dataset_dir / 'labels'\n",
        "        image_paths = sorted(images_dir.glob('*.nii'))#changed to .nii from .nii.gz for TESTING\n",
        "        label_paths = sorted(labels_dir.glob('*.nii'))\n",
        "\n",
        "        #FOR TESTING ONLY\n",
        "        image_paths *=5; label_paths *=5\n",
        "\n",
        "        assert len(image_paths) == len(label_paths)\n",
        "        self.dataset = []\n",
        "        for (image_path, label_path) in tqdm(zip(image_paths, label_paths), total=len(image_paths)):\n",
        "            subject = tio.Subject(\n",
        "                sample=tio.ScalarImage(image_path),\n",
        "                label =  tio.LabelMap(label_path))\n",
        "            self.dataset.append(subject)\n",
        "            time.sleep(0.1)\n",
        "\n",
        "    def setup(self, stage):\n",
        "        num_training_subjects = int(training_split_ratio * len(self.dataset))\n",
        "        num_validation_subjects = len(self.dataset) - num_training_subjects\n",
        "\n",
        "        num_split_subjects = num_training_subjects, num_validation_subjects\n",
        "        self.training_subjects, self.validation_subjects = torch.utils.data.random_split(modded_subjects, num_split_subjects)\n",
        "    \n",
        "    def train_dataloader(self):\n",
        "        training_transform = tio.Compose([\n",
        "                    tio.ToCanonical(),\n",
        "                    tio.Resample(RESAMPLE),\n",
        "                    tio.CropOrPad(self.image_shape),\n",
        "                    tio.RandomMotion(p=0.2),\n",
        "                    tio.RandomBiasField(p=0.3),\n",
        "                    tio.ZNormalization(masking_method=tio.ZNormalization.mean),\n",
        "                    tio.RescaleIntensity([0,1]),\n",
        "                    tio.RandomNoise(p=0.5),\n",
        "                    tio.RandomFlip(),\n",
        "                    tio.OneOf({\n",
        "                        tio.RandomAffine(): 0.8,\n",
        "                        tio.RandomElasticDeformation(): 0.2,\n",
        "                    }),\n",
        "                    tio.OneHot()])\n",
        "\n",
        "        validation_transform = tio.Compose([\n",
        "                    tio.ToCanonical(),\n",
        "                    tio.Resample(RESAMPLE),\n",
        "                    tio.CropOrPad(self.image_shape),\n",
        "                    tio.ZNormalization(masking_method=tio.ZNormalization.mean),\n",
        "                    tio.OneHot(),\n",
        "                ])\n",
        "        training_set = tio.SubjectsDataset(\n",
        "                            training_subjects, transform=training_transform)\n",
        "\n",
        "        training_loader = torch.utils.data.DataLoader(\n",
        "                            training_set,\n",
        "                            batch_size=self.train_batch_size,\n",
        "                            shuffle=True,\n",
        "                            num_workers=self.num_workers)\n",
        "        return training_loader\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        validation_transform = tio.Compose([\n",
        "                    tio.ToCanonical(),\n",
        "                    tio.Resample(4),\n",
        "                    tio.CropOrPad(shapey),\n",
        "                    tio.ZNormalization(masking_method=tio.ZNormalization.mean),\n",
        "                    tio.OneHot(),\n",
        "                ])\n",
        "        validation_set = tio.SubjectsDataset(\n",
        "                        validation_subjects, transform=validation_transform)\n",
        "      \n",
        "        validation_loader = torch.utils.data.DataLoader(\n",
        "                        validation_set,\n",
        "                        batch_size=self.val_batch_size,\n",
        "                        shuffle=True,\n",
        "                        num_workers=self.num_workers)\n",
        "        return validation_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import wandb\n",
        "# wandb.init(project=\"3dUnet\")\n",
        "wandb.config = {\n",
        "    #Model Parameters\n",
        "  \"num_encoding_blocks\":3,\n",
        "  \"out_channels_first_layer\":8,\n",
        "  \"activation\": \"PReLU\",\n",
        "    #Training Parameters\n",
        "  \"learning_rate\": 0.001,\n",
        "  \"epochs\": 10,\n",
        "  \"train_batch_size\": 4,\n",
        "  \"val_batch_size\":4\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "CHANNELS_DIMENSION = 1\n",
        "SPATIAL_DIMENSIONS = 2, 3, 4\n",
        "\n",
        "class Segmenter(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = UNet(\n",
        "        in_channels=1,\n",
        "        out_classes=6,\n",
        "        dimensions=3,\n",
        "        num_encoding_blocks=wandb.config['num_encoding_blocks'],\n",
        "        out_channels_first_layer=wandb.config['out_channels_first_layer'],\n",
        "        normalization='batch',\n",
        "        upsampling_type='linear',\n",
        "        padding=True,\n",
        "        activation=wandb.config['activation'])\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=wandb.config['learning_rate'])\n",
        "        return optimizer\n",
        "    def training_step(self, train_batch, batch_index):\n",
        "        inputs = train_batch['sample'][tio.DATA]\n",
        "        targets = train_batch['label'][tio.DATA]\n",
        "        with torch.enable_grad():\n",
        "            logits = self.model(inputs)\n",
        "            probabilities = F.softmax(logits, dim=CHANNELS_DIMENSION)\n",
        "            batch_losses = get_dice_loss(probabilities, targets.short())\n",
        "            batch_loss = batch_losses.mean()\n",
        "        self.log('train_loss', batch_loss.item())\n",
        "        return {'loss': batch_loss}\n",
        "   \n",
        "    def validation_step(self, val_batch, batch_index):\n",
        "        inputs, targets = prepare_batch(val_batch)\n",
        "        with torch.no_grad():\n",
        "            logits = self.model(inputs)\n",
        "            probabilities = F.softmax(logits, dim=CHANNELS_DIMENSION)\n",
        "            batch_losses = get_dice_loss(probabilities, targets.short())\n",
        "            batch_loss = batch_losses.mean()\n",
        "        self.log('val_loss', batch_loss.item())\n",
        "        return {'loss': batch_loss}\n",
        "    \n",
        "    def predict_step(self, batch, batch_idx, datloader_idx=0):\n",
        "        prediction = self(batch)\n",
        "        return prediction\n",
        "    # def backward(self, trainer, loss, optimizer, optimizer_idx):\n",
        "    #     loss.backward()\n",
        "    # def optimizer_step(self,epoch=None, batch_idx=None, optimizer=None, optimizer_idx=None, optimizer_closure=None, on_tpu=None, using_native_amp=None, using_lbfgs=None):\n",
        "    #     optimizer.step()\n",
        "    #     optimizer.closure()\n",
        "\n",
        "        \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5/5 [00:00<00:00,  9.78it/s]\n",
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "Running in `fast_dev_run` mode: will run the requested loop using 1 batch(es). Logging and checkpointing is suppressed.\n"
          ]
        }
      ],
      "source": [
        "module = CATDataModule()\n",
        "module.prepare_data(2,2)\n",
        "module.setup(0)\n",
        "mod = module.val_dataloader()\n",
        "model = Segmenter()\n",
        "trainer = pl.Trainer(fast_dev_run=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  | Name  | Type | Params\n",
            "-------------------------------\n",
            "0 | model | UNet | 246 K \n",
            "-------------------------------\n",
            "246 K     Trainable params\n",
            "0         Non-trainable params\n",
            "246 K     Total params\n",
            "0.985     Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0:   0%|          | 0/2 [00:00<?, ?it/s] LOSS TYPE: <class 'torch.Tensor'>\n",
            "Epoch 0:  50%|█████     | 1/2 [00:26<00:26, 26.42s/it, loss=0.924, v_num=]LOSS TYPE: <class 'torch.Tensor'>\n",
            "Epoch 0: 100%|██████████| 2/2 [00:37<00:00, 18.59s/it, loss=0.924, v_num=]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "e:\\Applications\\Anaconda\\envs\\swamphacks\\lib\\site-packages\\pytorch_lightning\\utilities\\data.py:98: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
            "  warning_cache.warn(\n",
            "`Trainer.fit` stopped: `max_steps=1` reached.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: 100%|██████████| 2/2 [00:37<00:00, 18.59s/it, loss=0.924, v_num=]\n"
          ]
        }
      ],
      "source": [
        "pred = trainer.fit(model, module.train_dataloader(), module.val_dataloader())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "d = module.train_dataloader()\n",
        "d = iter(d)\n",
        "sub =  next(d)\n",
        "inputs = sub['sample'][tio.DATA]\n",
        "# print(inputs)\n",
        "# tensor_to_video(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n = inputs.numpy()\n",
        "n = n[:,:,0,0,0]\n",
        "print(np.shape(n))\n",
        "print(n[:,0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "inputs.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Unconverted Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_model_and_optimizer(device):\n",
        "    model = UNet(\n",
        "        in_channels=1,\n",
        "        out_classes=6,\n",
        "        dimensions=3,\n",
        "        num_encoding_blocks=wandb.config['num_encoding_blocks'],\n",
        "        out_channels_first_layer=wandb.config['out_channels_first_layer'],\n",
        "        normalization='batch',\n",
        "        upsampling_type='linear',\n",
        "        padding=True,\n",
        "        activation=wandb.config['activation'],\n",
        "    ).to(device)\n",
        "    # print(model.parameters)\n",
        "    \n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=wandb.config['learning_rate'])\n",
        "    return model, optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title (Deep learning functions, double-click here to expand)\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else 'cpu'\n",
        "CHANNELS_DIMENSION = 1\n",
        "SPATIAL_DIMENSIONS = 2, 3, 4\n",
        "\n",
        "class Action(enum.Enum):\n",
        "    TRAIN = 'Training'\n",
        "    VALIDATE = 'Validation'\n",
        "\n",
        "# def prepare_batch(batch, device):\n",
        "#     if device != None:\n",
        "#         inputs = batch['sample'][tio.DATA].to(device)\n",
        "#         targets = batch['label'][tio.DATA].to(device)\n",
        "#     else:\n",
        "#         inputs = batch['sample'][tio.DATA]\n",
        "#         targets = batch['label'][tio.DATA]\n",
        "\n",
        "#     print(f\"DTYPE: {type(inputs)}\")\n",
        "#     return inputs, targets\n",
        "\n",
        "def get_dice_score(output, target, epsilon=1e-9):\n",
        "    p0 = output\n",
        "    g0 = target\n",
        "    p1 = 1 - p0\n",
        "    g1 = 1 - g0\n",
        "    tp = (p0 * g0).sum(dim=SPATIAL_DIMENSIONS)\n",
        "    fp = (p0 * g1).sum(dim=SPATIAL_DIMENSIONS)\n",
        "    fn = (p1 * g0).sum(dim=SPATIAL_DIMENSIONS)\n",
        "    num = 2 * tp\n",
        "    denom = 2 * tp + fp + fn + epsilon\n",
        "    dice_score = num / denom\n",
        "    return dice_score\n",
        "\n",
        "def get_dice_loss(output, target):\n",
        "    return 1 - get_dice_score(output, target)\n",
        "\n",
        "def get_model_and_optimizer(device):\n",
        "    model = UNet(\n",
        "        in_channels=1,\n",
        "        out_classes=6,\n",
        "        dimensions=3,\n",
        "        num_encoding_blocks=wandb.config['num_encoding_blocks'],\n",
        "        out_channels_first_layer=wandb.config['out_channels_first_layer'],\n",
        "        normalization='batch',\n",
        "        upsampling_type='linear',\n",
        "        padding=True,\n",
        "        activation=wandb.config['activation'],\n",
        "    ).to(device)\n",
        "    # print(model.parameters)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=wandb.config['learning_rate'])\n",
        "    return model, optimizer\n",
        "\n",
        "def run_epoch(epoch_idx, action, loader, model, optimizer):\n",
        "    is_training = action == Action.TRAIN\n",
        "    epoch_losses = []\n",
        "    times = []\n",
        "    model.train(is_training)\n",
        "    for batch_idx, batch in enumerate(tqdm(loader)):\n",
        "        inputs, targets = prepare_batch(batch, device)\n",
        "        optimizer.zero_grad()\n",
        "        with torch.set_grad_enabled(is_training):\n",
        "            logits = model(inputs)\n",
        "            print(f\"dtype: {inputs[0][0][0]}\")\n",
        "            probabilities = F.softmax(logits, dim=CHANNELS_DIMENSION)\n",
        "            batch_losses = get_dice_loss(probabilities, targets.short())\n",
        "            batch_loss = batch_losses.mean()\n",
        "            if is_training:\n",
        "                batch_loss.backward()\n",
        "                optimizer.step()\n",
        "            times.append(time.time())\n",
        "            epoch_losses.append(batch_loss)\n",
        "    epoch_losses = np.array(epoch_losses)\n",
        "    print(f'{action.value} mean loss: {epoch_losses.mean()}')\n",
        "    return times, epoch_losses\n",
        "\n",
        "def train(num_epochs, training_loader, validation_loader, model, optimizer, weights_stem):\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "   \n",
        "\n",
        "    # val_losses.append(run_epoch(0, Action.VALIDATE, validation_loader, model, optimizer))\n",
        "    for epoch_idx in range(1, num_epochs + 1):\n",
        "\n",
        "\n",
        "        print('Starting epoch', epoch_idx)\n",
        "        train_loss = run_epoch(epoch_idx, Action.TRAIN, training_loader, model, optimizer)\n",
        "        train_losses.append(train_loss)\n",
        "        val_loss = run_epoch(epoch_idx, Action.VALIDATE, validation_loader, model, optimizer)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        wandb.log({\"val_loss\": val_loss})\n",
        "        wandb.log({\"train_loss\": train_loss})\n",
        "\n",
        "        torch.save(model.state_dict(), f'{weights_stem}_epoch_{epoch_idx}.pth')\n",
        "    return np.array(train_losses), np.array(val_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "module.training_subjects[1].plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tens = training_subjects[0].sample.data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "ind = random.randint(0,tens.size()[1])\n",
        "slice = tens[:, ind,:,:]\n",
        "import torchvision.transforms.functional as Ft\n",
        "img = Ft.to_pil_image(slice)\n",
        "\n",
        "display(img)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "edited_tens = "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#plot 1 tensor, verify color space redux works"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dic = {}\n",
        "dic['sample'] = it['sample'].data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWbWAyFNTj74"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import wandb\n",
        "wandb.init(project=\"3dUnet\")\n",
        "wandb.config = {\n",
        "    #Model Parameters\n",
        "  \"num_encoding_blocks\":3,\n",
        "  \"out_channels_first_layer\":8,\n",
        "  \"activation\": \"PReLU\",\n",
        "    #Training Parameters\n",
        "  \"learning_rate\": 0.001,\n",
        "  \"epochs\": 10,\n",
        "  \"train_batch_size\": 4,\n",
        "  \"val_batch_size\":4\n",
        "  \n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if (torch.cuda.is_available()):\n",
        "    print(\"Cuda is available\")\n",
        "model, optimizer = get_model_and_optimizer(device)\n",
        "# wandb.watch(model)\n",
        "weights_path = 'whole_image_state_dict.pth'\n",
        "training_loader = torch.utils.data.DataLoader(\n",
        "    training_set,\n",
        "    batch_size=wandb.config['train_batch_size'],\n",
        "    shuffle=True,\n",
        "    num_workers=1,\n",
        ")\n",
        "\n",
        "validation_loader = torch.utils.data.DataLoader(\n",
        "    validation_set,\n",
        "    batch_size=wandb.config['val_batch_size'],\n",
        "    num_workers=1,\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "LFpQsUAyPxeA",
        "outputId": "4c4eef66-1ef6-423e-ee9d-63bc583f40c1"
      },
      "outputs": [],
      "source": [
        "weights_stem = 'whole_images'\n",
        "train_losses, val_losses = train(wandb.config['epochs'], training_loader, validation_loader, model, optimizer, weights_stem)\n",
        "checkpoint = {\n",
        "    'train_losses': train_losses,\n",
        "    'val_losses': val_losses,\n",
        "    'weights': model.state_dict(),\n",
        "}\n",
        "torch.save(checkpoint, weights_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sS4xOEbwhKVU"
      },
      "source": [
        "#### Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def staty(image):\n",
        "    print(f\"This image is {image.size[0]} by {image.size[1]}\")\n",
        "    arr = np.array(image)\n",
        "    print(f\"Mean pixel value is {np.mean(arr)}\")\n",
        "    print(f\"Median pixel value is {np.median(arr)}\")\n",
        "    print(f\"Minimum is {np.min(arr)}, Max is {np.max(arr)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torchvision.transforms as tv\n",
        "convert = tv.Compose([tv.ToPILImage()])\n",
        "slices = []\n",
        "for i in range(6):\n",
        "    slices.append(probabilities[:,i,24,:,:])\n",
        "\n",
        "# im = convert(slices[1])\n",
        "# imshow(im)\n",
        "# stats(im)\n",
        "affine = batch['sample'][tio.AFFINE][0].numpy()\n",
        "ind = 24\n",
        "\n",
        "pl1 = inputs.cpu()\n",
        "pl2 = targets.cpu()\n",
        "probs = probabilities.cpu()\n",
        "subject = tio.Subject(\n",
        "    scan=tio.ScalarImage(tensor=pl[:,:,ind,:,:]),\n",
        "    label1=tio.LabelMap(tensor=pl2[:,:,ind,:,:],),\n",
        "    predicted=tio.ScalarImage(tensor=probs[:,:,ind,:,:]),\n",
        ")\n",
        "subject.plot(figsize=(9, 8))#, cmap_dict={'predicted': 'RdBu_r'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "id": "U-5ZGq5UZ5QK",
        "outputId": "28d5b7d9-7e1e-46dd-c71f-df30880fbd87"
      },
      "outputs": [],
      "source": [
        "# batch = next(iter(validation_loader))\n",
        "batch = next(iter(training_loader))\n",
        "\n",
        "model.eval()\n",
        "inputs, targets = prepare_batch(batch, device)\n",
        "FIRST = 0\n",
        "FOREGROUND = 1\n",
        "with torch.no_grad():\n",
        "    probabilities = model(inputs).softmax(dim=1)[:, FOREGROUND:].cpu()\n",
        "affine = batch['sample'][tio.AFFINE][0].numpy()\n",
        "subject = tio.Subject(\n",
        "    mri=tio.ScalarImage(tensor=batch['sample'][tio.DATA][FIRST], affine=affine),\n",
        "    label=tio.LabelMap(tensor=batch['label'][tio.DATA][FIRST], affine=affine),\n",
        "    predicted=tio.ScalarImage(tensor=probabilities[FIRST], affine=affine),\n",
        ")\n",
        "subject.plot(figsize=(9, 8), cmap_dict={'predicted': 'RdBu_r'})"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "TorchIO tutorial.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.10 ('swamphacks')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "6a4907715359045b8a2ace855fc06a352fe7b25f30531dd2e3f9c93331a0a772"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
