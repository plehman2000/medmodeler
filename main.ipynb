{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXkCTL7DAQaR"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "b0NdJiFW3Uy7",
        "outputId": "a975549f-6a17-4dda-9faa-23693c3c1ab3"
      },
      "outputs": [],
      "source": [
        "#RUNNING ON ENVIRONMENT 'UNEXT'\n",
        "# Config\n",
        "seed = 42  # for reproducibility\n",
        "training_split_ratio = 0.9  # use 90% of samples for training, 10% for testing\n",
        "num_epochs = 5\n",
        "\n",
        "# If the following values are False, the models will be downloaded and not computed\n",
        "compute_histograms = False\n",
        "train_whole_images = False \n",
        "train_patches = False\n",
        "\n",
        "from IPython.display import display\n",
        "import enum\n",
        "import time\n",
        "import random\n",
        "import multiprocessing\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchio as tio\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "from unet import UNet\n",
        "from scipy import stats\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from IPython.display import display\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "num_workers = multiprocessing.cpu_count()\n",
        "plt.rcParams['figure.figsize'] = 12, 6\n",
        "\n",
        "print('Last run on', time.ctime())\n",
        "print('TorchIO version:', tio.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "dataset_dir = Path(os.getcwd() + \"/DATA/\")\n",
        "images_dir = dataset_dir / 'images'\n",
        "labels_dir = dataset_dir / 'labels'\n",
        "image_paths = sorted(images_dir.glob('*.nii'))\n",
        "label_paths = sorted(labels_dir.glob('*.nii'))\n",
        "print(len(label_paths))\n",
        "print(len(image_paths))\n",
        "\n",
        "assert len(image_paths) == len(label_paths)\n",
        "\n",
        "\n",
        "import time\n",
        "modded_subjects = []\n",
        "\n",
        "for (image_path, label_path) in tqdm((zip(image_paths, label_paths))):\n",
        "    subject = tio.Subject(\n",
        "        sample=tio.ScalarImage(image_path),\n",
        "        label =  tio.LabelMap(label_path)\n",
        "    )\n",
        "    # print(subject['label1'].tensor[:,:,:,0].shape)\n",
        "\n",
        "    modded_subjects.append(subject)\n",
        "    time.sleep(0.5)\n",
        "\n",
        "modded_subjects = modded_subjects*5\n",
        "\n",
        "dataset = tio.SubjectsDataset(modded_subjects*5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset[0].plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mul = 5\n",
        "shapey = (48, mul*60, mul*48)\n",
        "RESAMPLE_SPACE = 1 #RETAINS BASE RESOLUTION\n",
        "training_transform = tio.Compose([\n",
        "    tio.ToCanonical(),\n",
        "    tio.Resample(RESAMPLE_SPACE),\n",
        "    tio.RandomMotion(p=0.2),\n",
        "    tio.RandomBiasField(p=0.3),\n",
        "    tio.ZNormalization(masking_method=tio.ZNormalization.mean),\n",
        "    tio.RandomNoise(p=0.5),\n",
        "    tio.RandomFlip(),\n",
        "    tio.OneOf({\n",
        "        tio.RandomAffine(): 0.8,\n",
        "        tio.RandomElasticDeformation(): 0.2,\n",
        "    }),\n",
        "    tio.CropOrPad(shapey), #should this be after the flip?\n",
        "    tio.RescaleIntensity((0,1)),\n",
        "    tio.OneHot(),\n",
        "])\n",
        "\n",
        "validation_transform = tio.Compose([\n",
        "    tio.ToCanonical(),\n",
        "    tio.Resample(RESAMPLE_SPACE),\n",
        "    tio.CropOrPad(shapey),\n",
        "    tio.ZNormalization(masking_method=tio.ZNormalization.mean),\n",
        "    tio.OneHot(),\n",
        "])\n",
        "\n",
        "\n",
        "dataset = tio.SubjectsDataset(modded_subjects) #can multiply size because of augmentation\n",
        "\n",
        "num_subjects = len(dataset)\n",
        "num_training_subjects = int(training_split_ratio * num_subjects)\n",
        "num_validation_subjects = num_subjects - num_training_subjects\n",
        "\n",
        "num_split_subjects = num_training_subjects, num_validation_subjects\n",
        "print(num_split_subjects)\n",
        "training_subjects, validation_subjects = torch.utils.data.random_split(modded_subjects, num_split_subjects)\n",
        "\n",
        "training_set = tio.SubjectsDataset(\n",
        "    training_subjects, transform=training_transform)\n",
        "\n",
        "validation_set = tio.SubjectsDataset(\n",
        "    validation_subjects, transform=validation_transform)\n",
        "\n",
        "training_set_raw = [] \n",
        "val_set_raw = []\n",
        "\n",
        "print('Training set:', len(training_set), 'subjects')\n",
        "print('Validation set:', len(validation_set), 'subjects')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Lightning Converted Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pytorch_lightning as pl\n",
        "import os\n",
        "import time\n",
        "DATA_PATH = Path(os.getcwd() + \"/DATA/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#non class functions\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "  \n",
        "import torchvision.transforms.functional as Ft\n",
        "\n",
        "def prepare_batch(batch):\n",
        "    inputs = batch['sample'][tio.DATA]\n",
        "    targets = batch['label'][tio.DATA]\n",
        "    return inputs, targets\n",
        "def get_dice_score(output, target, epsilon=1e-9):\n",
        "    print(f\"Output: {type(output)}\")\n",
        "    print(f\"Target: {type(target)}\")\n",
        "    p0 = output\n",
        "    g0 = target\n",
        "    p1 = 1 - p0\n",
        "    g1 = 1 - g0\n",
        "    tp = (p0 * g0).sum(dim=SPATIAL_DIMENSIONS)\n",
        "    fp = (p0 * g1).sum(dim=SPATIAL_DIMENSIONS)\n",
        "    fn = (p1 * g0).sum(dim=SPATIAL_DIMENSIONS)\n",
        "    num = 2 * tp\n",
        "    denom = 2 * tp + fp + fn + epsilon\n",
        "    dice_score = num / denom\n",
        "    return dice_score\n",
        "\n",
        "def get_dice_loss(output, target):\n",
        "    loss = 1 - get_dice_score(output, target)\n",
        "    if type(loss) is None:\n",
        "        return -1\n",
        "    return loss\n",
        "\n",
        "\n",
        "\n",
        "def tensor_to_video(inputs, name=\"TEST\"):\n",
        "  slices = []\n",
        "  print()\n",
        "  for i in range(inputs.size()[2]):\n",
        "      slice = inputs[0, :,i,:,:];img = Ft.to_pil_image(slice, mode='L')\n",
        "      # print(slice.size())\n",
        "      slices.append(img)\n",
        "  fourcc = cv2.VideoWriter_fourcc(*'mp4v') \n",
        "  video = cv2.VideoWriter(f'{name}.mp4', fourcc, inputs.size()[2], (inputs.size()[4]  , inputs.size()[3]))\n",
        "\n",
        "  for j in slices:\n",
        "    j = np.array(j)\n",
        "    shape = np.shape(j)\n",
        "    opencvImage = cv2.cvtColor(j, cv2.COLOR_GRAY2BGR)\n",
        "    video.write(opencvImage)\n",
        "\n",
        "  cv2.destroyAllWindows()\n",
        "  video.release()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "RESAMPLE = 1 #4\n",
        "class CATDataModule(pl.LightningDataModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        image_mul = 4\n",
        "        self.image_shape = (48, image_mul*60, image_mul*48)\n",
        "        self.training_subjects = []\n",
        "        self.validation_subjects = []\n",
        "        self.train_batch_size = 1\n",
        "        self.val_batch_size = 1\n",
        "        self.dataset = []\n",
        "        self.num_workers = 8\n",
        "\n",
        "    def prepare_data(self, train_batch_size, val_batch_size):\n",
        "        dataset_dir = DATA_PATH\n",
        "        images_dir = dataset_dir / 'images'\n",
        "        labels_dir = dataset_dir / 'labels'\n",
        "        image_paths = sorted(images_dir.glob('*.nii'))#changed to .nii from .nii.gz for TESTING\n",
        "        label_paths = sorted(labels_dir.glob('*.nii'))\n",
        "\n",
        "        #FOR TESTING ONLY\n",
        "        image_paths *=5; label_paths *=5\n",
        "\n",
        "        assert len(image_paths) == len(label_paths)\n",
        "        self.dataset = []\n",
        "        for (image_path, label_path) in tqdm(zip(image_paths, label_paths), total=len(image_paths)):\n",
        "            subject = tio.Subject(\n",
        "                sample=tio.ScalarImage(image_path),\n",
        "                label =  tio.LabelMap(label_path))\n",
        "            self.dataset.append(subject)\n",
        "            time.sleep(0.1)\n",
        "\n",
        "    def setup(self, stage):\n",
        "        num_training_subjects = int(training_split_ratio * len(self.dataset))\n",
        "        num_validation_subjects = len(self.dataset) - num_training_subjects\n",
        "\n",
        "        num_split_subjects = num_training_subjects, num_validation_subjects\n",
        "        self.training_subjects, self.validation_subjects = torch.utils.data.random_split(modded_subjects, num_split_subjects)\n",
        "    \n",
        "    def train_dataloader(self):\n",
        "        training_transform = tio.Compose([\n",
        "                    tio.ToCanonical(),\n",
        "                    tio.Resample(RESAMPLE),\n",
        "                    tio.CropOrPad(self.image_shape),\n",
        "                    tio.RandomMotion(p=0.2),\n",
        "                    tio.RandomBiasField(p=0.3),\n",
        "                    tio.ZNormalization(masking_method=tio.ZNormalization.mean),\n",
        "                    tio.RescaleIntensity([0,1]),\n",
        "                    tio.RandomNoise(p=0.5),\n",
        "                    tio.RandomFlip(),\n",
        "                    tio.OneOf({\n",
        "                        tio.RandomAffine(): 0.8,\n",
        "                        tio.RandomElasticDeformation(): 0.2,\n",
        "                    }),\n",
        "                    tio.OneHot()])\n",
        "\n",
        "        validation_transform = tio.Compose([\n",
        "                    tio.ToCanonical(),\n",
        "                    tio.Resample(RESAMPLE),\n",
        "                    tio.CropOrPad(self.image_shape),\n",
        "                    tio.ZNormalization(masking_method=tio.ZNormalization.mean),\n",
        "                    tio.OneHot(),\n",
        "                ])\n",
        "        training_set = tio.SubjectsDataset(\n",
        "                            training_subjects, transform=training_transform)\n",
        "\n",
        "        training_loader = torch.utils.data.DataLoader(\n",
        "                            training_set,\n",
        "                            batch_size=self.train_batch_size,\n",
        "                            shuffle=True,\n",
        "                            num_workers=self.num_workers)\n",
        "        return training_loader\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        validation_transform = tio.Compose([\n",
        "                    tio.ToCanonical(),\n",
        "                    tio.Resample(4),\n",
        "                    tio.CropOrPad(shapey),\n",
        "                    tio.ZNormalization(masking_method=tio.ZNormalization.mean),\n",
        "                    tio.OneHot(),\n",
        "                ])\n",
        "        validation_set = tio.SubjectsDataset(\n",
        "                        validation_subjects, transform=validation_transform)\n",
        "      \n",
        "        validation_loader = torch.utils.data.DataLoader(\n",
        "                        validation_set,\n",
        "                        batch_size=self.val_batch_size,\n",
        "                        shuffle=True,\n",
        "                        num_workers=self.num_workers)\n",
        "        return validation_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import wandb\n",
        "# wandb.init(project=\"3dUnet\")\n",
        "wandb.config = {\n",
        "    #Model Parameters\n",
        "  \"num_encoding_blocks\":3,\n",
        "  \"out_channels_first_layer\":8,\n",
        "  \"activation\": \"PReLU\",\n",
        "    #Training Parameters\n",
        "  \"learning_rate\": 0.001,\n",
        "  \"epochs\": 10,\n",
        "  \"train_batch_size\": 4,\n",
        "  \"val_batch_size\":4\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "CHANNELS_DIMENSION = 2\n",
        "SPATIAL_DIMENSIONS = 3, 4, 5\n",
        "\n",
        "class Segmenter(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = UNet(\n",
        "        in_channels=1,\n",
        "        out_classes=6,\n",
        "        dimensions=3,\n",
        "        num_encoding_blocks=wandb.config['num_encoding_blocks'],\n",
        "        out_channels_first_layer=wandb.config['out_channels_first_layer'],\n",
        "        normalization='batch',\n",
        "        upsampling_type='linear',\n",
        "        padding=True,\n",
        "        activation=wandb.config['activation'])\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=wandb.config['learning_rate'])\n",
        "        return optimizer\n",
        "    def training_step(self, train_batch, batch_index):\n",
        "        inputs = train_batch['sample'][tio.DATA]\n",
        "        targets = train_batch['label'][tio.DATA]\n",
        "        with torch.enable_grad():\n",
        "            logits = self.model(inputs)\n",
        "            probabilities = F.softmax(logits, dim=CHANNELS_DIMENSION)\n",
        "            batch_losses = get_dice_loss(probabilities, targets.short())\n",
        "            batch_loss = batch_losses.mean()\n",
        "        self.log('train_loss', batch_loss.item())\n",
        "        loss = batch_loss.item()\n",
        "        if loss is None:\n",
        "            return -1\n",
        "        return -1\n",
        "   \n",
        "    def validation_step(self, val_batch, batch_index):\n",
        "        inputs, targets = prepare_batch(val_batch)\n",
        "        with torch.no_grad():\n",
        "            logits = self.model(inputs)\n",
        "            probabilities = F.softmax(logits, dim=CHANNELS_DIMENSION)\n",
        "            batch_losses = get_dice_loss(probabilities, targets.short())\n",
        "            batch_loss = batch_losses.mean()\n",
        "        self.log('val_loss', batch_loss.item())\n",
        "        loss = batch_loss.item()\n",
        "        if loss is None:\n",
        "            return -1\n",
        "        return -1\n",
        "    \n",
        "    def predict_step(self, batch, batch_idx, datloader_idx=0):\n",
        "        prediction = self(batch)\n",
        "        return prediction\n",
        "    def backward(self, trainer, loss, optimizer, optimizer_idx):\n",
        "        loss.backward()\n",
        "    # def optimizer_step(self,epoch=None, batch_idx=None, optimizer=None, optimizer_idx=None, optimizer_closure=None, on_tpu=None, using_native_amp=None, using_lbfgs=None):\n",
        "    #     optimizer.step()\n",
        "    #     optimizer.closure)0-\n",
        "\n",
        "        \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "module = CATDataModule()\n",
        "module.prepare_data(2,1)\n",
        "module.setup(0)\n",
        "mod = module.val_dataloader()\n",
        "model = Segmenter()\n",
        "trainer = pl.Trainer(fast_dev_run=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "e:\\Applications\\Anaconda\\envs\\swamphacks\\lib\\site-packages\\unet\\decoding.py:146: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  half_crop = crop // 2\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output: <class 'torch.Tensor'>\n",
            "Target: <class 'torch.Tensor'>\n"
          ]
        },
        {
          "ename": "IndexError",
          "evalue": "Dimension out of range (expected to be in range of [-5, 4], but got 5)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32me:\\Projects\\Lab\\MRI\\main.ipynb Cell 13\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Projects/Lab/MRI/main.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m pred \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mfit(model, module\u001b[39m.\u001b[39;49mtrain_dataloader(), module\u001b[39m.\u001b[39;49mval_dataloader())\n",
            "File \u001b[1;32me:\\Applications\\Anaconda\\envs\\swamphacks\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:696\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    678\u001b[0m \u001b[39mRuns the full optimization routine.\u001b[39;00m\n\u001b[0;32m    679\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    693\u001b[0m \u001b[39m    datamodule: An instance of :class:`~pytorch_lightning.core.datamodule.LightningDataModule`.\u001b[39;00m\n\u001b[0;32m    694\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    695\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m model\n\u001b[1;32m--> 696\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[0;32m    697\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[0;32m    698\u001b[0m )\n",
            "File \u001b[1;32me:\\Applications\\Anaconda\\envs\\swamphacks\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:650\u001b[0m, in \u001b[0;36mTrainer._call_and_handle_interrupt\u001b[1;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    648\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    649\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 650\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    651\u001b[0m \u001b[39m# TODO(awaelchli): Unify both exceptions below, where `KeyboardError` doesn't re-raise\u001b[39;00m\n\u001b[0;32m    652\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m \u001b[39mas\u001b[39;00m exception:\n",
            "File \u001b[1;32me:\\Applications\\Anaconda\\envs\\swamphacks\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:737\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    733\u001b[0m ckpt_path \u001b[39m=\u001b[39m ckpt_path \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresume_from_checkpoint\n\u001b[0;32m    734\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__set_ckpt_path(\n\u001b[0;32m    735\u001b[0m     ckpt_path, model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    736\u001b[0m )\n\u001b[1;32m--> 737\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mckpt_path)\n\u001b[0;32m    739\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[0;32m    740\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
            "File \u001b[1;32me:\\Applications\\Anaconda\\envs\\swamphacks\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1168\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m   1164\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mrestore_training_state()\n\u001b[0;32m   1166\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mresume_end()\n\u001b[1;32m-> 1168\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[0;32m   1170\u001b[0m log\u001b[39m.\u001b[39mdetail(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1171\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_teardown()\n",
            "File \u001b[1;32me:\\Applications\\Anaconda\\envs\\swamphacks\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1254\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1252\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[0;32m   1253\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_predict()\n\u001b[1;32m-> 1254\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_train()\n",
            "File \u001b[1;32me:\\Applications\\Anaconda\\envs\\swamphacks\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1285\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1282\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_loop\u001b[39m.\u001b[39mtrainer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\n\u001b[0;32m   1284\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[1;32m-> 1285\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_loop\u001b[39m.\u001b[39;49mrun()\n",
            "File \u001b[1;32me:\\Applications\\Anaconda\\envs\\swamphacks\\lib\\site-packages\\pytorch_lightning\\loops\\loop.py:200\u001b[0m, in \u001b[0;36mLoop.run\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    199\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madvance(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[0;32m    202\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
            "File \u001b[1;32me:\\Applications\\Anaconda\\envs\\swamphacks\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:270\u001b[0m, in \u001b[0;36mFitLoop.advance\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_fetcher\u001b[39m.\u001b[39msetup(\n\u001b[0;32m    267\u001b[0m     dataloader, batch_to_device\u001b[39m=\u001b[39mpartial(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39m_call_strategy_hook, \u001b[39m\"\u001b[39m\u001b[39mbatch_to_device\u001b[39m\u001b[39m\"\u001b[39m, dataloader_idx\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m    268\u001b[0m )\n\u001b[0;32m    269\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_epoch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 270\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_fetcher)\n",
            "File \u001b[1;32me:\\Applications\\Anaconda\\envs\\swamphacks\\lib\\site-packages\\pytorch_lightning\\loops\\loop.py:200\u001b[0m, in \u001b[0;36mLoop.run\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    199\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madvance(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[0;32m    202\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
            "File \u001b[1;32me:\\Applications\\Anaconda\\envs\\swamphacks\\lib\\site-packages\\pytorch_lightning\\loops\\epoch\\training_epoch_loop.py:203\u001b[0m, in \u001b[0;36mTrainingEpochLoop.advance\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_started()\n\u001b[0;32m    202\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_batch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 203\u001b[0m         batch_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_loop\u001b[39m.\u001b[39;49mrun(kwargs)\n\u001b[0;32m    205\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_processed()\n\u001b[0;32m    207\u001b[0m \u001b[39m# update non-plateau LR schedulers\u001b[39;00m\n\u001b[0;32m    208\u001b[0m \u001b[39m# update epoch-interval ones only when we are at the end of training epoch\u001b[39;00m\n",
            "File \u001b[1;32me:\\Applications\\Anaconda\\envs\\swamphacks\\lib\\site-packages\\pytorch_lightning\\loops\\loop.py:200\u001b[0m, in \u001b[0;36mLoop.run\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    199\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madvance(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[0;32m    202\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
            "File \u001b[1;32me:\\Applications\\Anaconda\\envs\\swamphacks\\lib\\site-packages\\pytorch_lightning\\loops\\batch\\training_batch_loop.py:87\u001b[0m, in \u001b[0;36mTrainingBatchLoop.advance\u001b[1;34m(self, kwargs)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mlightning_module\u001b[39m.\u001b[39mautomatic_optimization:\n\u001b[0;32m     84\u001b[0m     optimizers \u001b[39m=\u001b[39m _get_active_optimizers(\n\u001b[0;32m     85\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39moptimizers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39moptimizer_frequencies, kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mbatch_idx\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m0\u001b[39m)\n\u001b[0;32m     86\u001b[0m     )\n\u001b[1;32m---> 87\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer_loop\u001b[39m.\u001b[39;49mrun(optimizers, kwargs)\n\u001b[0;32m     88\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     89\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmanual_loop\u001b[39m.\u001b[39mrun(kwargs)\n",
            "File \u001b[1;32me:\\Applications\\Anaconda\\envs\\swamphacks\\lib\\site-packages\\pytorch_lightning\\loops\\loop.py:200\u001b[0m, in \u001b[0;36mLoop.run\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    199\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madvance(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[0;32m    202\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
            "File \u001b[1;32me:\\Applications\\Anaconda\\envs\\swamphacks\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\optimizer_loop.py:201\u001b[0m, in \u001b[0;36mOptimizerLoop.advance\u001b[1;34m(self, optimizers, kwargs)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39madvance\u001b[39m(\u001b[39mself\u001b[39m, optimizers: List[Tuple[\u001b[39mint\u001b[39m, Optimizer]], kwargs: OrderedDict) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:  \u001b[39m# type: ignore[override]\u001b[39;00m\n\u001b[0;32m    199\u001b[0m     kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_kwargs(kwargs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer_idx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hiddens)\n\u001b[1;32m--> 201\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_optimization(kwargs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizers[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptim_progress\u001b[39m.\u001b[39;49moptimizer_position])\n\u001b[0;32m    202\u001b[0m     \u001b[39mif\u001b[39;00m result\u001b[39m.\u001b[39mloss \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    203\u001b[0m         \u001b[39m# automatic optimization assumes a loss needs to be returned for extras to be considered as the batch\u001b[39;00m\n\u001b[0;32m    204\u001b[0m         \u001b[39m# would be skipped otherwise\u001b[39;00m\n\u001b[0;32m    205\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer_idx] \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39masdict()\n",
            "File \u001b[1;32me:\\Applications\\Anaconda\\envs\\swamphacks\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\optimizer_loop.py:248\u001b[0m, in \u001b[0;36mOptimizerLoop._run_optimization\u001b[1;34m(self, kwargs, optimizer)\u001b[0m\n\u001b[0;32m    240\u001b[0m         closure()\n\u001b[0;32m    242\u001b[0m \u001b[39m# ------------------------------\u001b[39;00m\n\u001b[0;32m    243\u001b[0m \u001b[39m# BACKWARD PASS\u001b[39;00m\n\u001b[0;32m    244\u001b[0m \u001b[39m# ------------------------------\u001b[39;00m\n\u001b[0;32m    245\u001b[0m \u001b[39m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[0;32m    246\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    247\u001b[0m     \u001b[39m# the `batch_idx` is optional with inter-batch parallelism\u001b[39;00m\n\u001b[1;32m--> 248\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizer_step(optimizer, opt_idx, kwargs\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mbatch_idx\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m0\u001b[39;49m), closure)\n\u001b[0;32m    250\u001b[0m result \u001b[39m=\u001b[39m closure\u001b[39m.\u001b[39mconsume_result()\n\u001b[0;32m    252\u001b[0m \u001b[39mif\u001b[39;00m result\u001b[39m.\u001b[39mloss \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    253\u001b[0m     \u001b[39m# if no result, user decided to skip optimization\u001b[39;00m\n\u001b[0;32m    254\u001b[0m     \u001b[39m# otherwise update running loss + reset accumulated loss\u001b[39;00m\n\u001b[0;32m    255\u001b[0m     \u001b[39m# TODO: find proper way to handle updating running loss\u001b[39;00m\n",
            "File \u001b[1;32me:\\Applications\\Anaconda\\envs\\swamphacks\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\optimizer_loop.py:358\u001b[0m, in \u001b[0;36mOptimizerLoop._optimizer_step\u001b[1;34m(self, optimizer, opt_idx, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[0;32m    355\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim_progress\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep\u001b[39m.\u001b[39mincrement_ready()\n\u001b[0;32m    357\u001b[0m \u001b[39m# model hook\u001b[39;00m\n\u001b[1;32m--> 358\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49m_call_lightning_module_hook(\n\u001b[0;32m    359\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39moptimizer_step\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    360\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mcurrent_epoch,\n\u001b[0;32m    361\u001b[0m     batch_idx,\n\u001b[0;32m    362\u001b[0m     optimizer,\n\u001b[0;32m    363\u001b[0m     opt_idx,\n\u001b[0;32m    364\u001b[0m     train_step_and_backward_closure,\n\u001b[0;32m    365\u001b[0m     on_tpu\u001b[39m=\u001b[39;49m\u001b[39misinstance\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49maccelerator, TPUAccelerator),\n\u001b[0;32m    366\u001b[0m     using_native_amp\u001b[39m=\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mamp_backend \u001b[39m==\u001b[39;49m AMPType\u001b[39m.\u001b[39;49mNATIVE),\n\u001b[0;32m    367\u001b[0m     using_lbfgs\u001b[39m=\u001b[39;49mis_lbfgs,\n\u001b[0;32m    368\u001b[0m )\n\u001b[0;32m    370\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m should_accumulate:\n\u001b[0;32m    371\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim_progress\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep\u001b[39m.\u001b[39mincrement_completed()\n",
            "File \u001b[1;32me:\\Applications\\Anaconda\\envs\\swamphacks\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1552\u001b[0m, in \u001b[0;36mTrainer._call_lightning_module_hook\u001b[1;34m(self, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1549\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m hook_name\n\u001b[0;32m   1551\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[LightningModule]\u001b[39m\u001b[39m{\u001b[39;00mpl_module\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[1;32m-> 1552\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1554\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m   1555\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
            "File \u001b[1;32me:\\Applications\\Anaconda\\envs\\swamphacks\\lib\\site-packages\\pytorch_lightning\\core\\module.py:1673\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[1;34m(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs)\u001b[0m\n\u001b[0;32m   1591\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimizer_step\u001b[39m(\n\u001b[0;32m   1592\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   1593\u001b[0m     epoch: \u001b[39mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1600\u001b[0m     using_lbfgs: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m   1601\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1602\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1603\u001b[0m \u001b[39m    Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[0;32m   1604\u001b[0m \u001b[39m    each optimizer.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1671\u001b[0m \n\u001b[0;32m   1672\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1673\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep(closure\u001b[39m=\u001b[39;49moptimizer_closure)\n",
            "File \u001b[1;32me:\\Applications\\Anaconda\\envs\\swamphacks\\lib\\site-packages\\pytorch_lightning\\core\\optimizer.py:168\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[1;34m(self, closure, **kwargs)\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[39mraise\u001b[39;00m MisconfigurationException(\u001b[39m\"\u001b[39m\u001b[39mWhen `optimizer.step(closure)` is called, the closure should be callable\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    167\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_strategy \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 168\u001b[0m step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_strategy\u001b[39m.\u001b[39moptimizer_step(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_idx, closure, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    170\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_on_after_step()\n\u001b[0;32m    172\u001b[0m \u001b[39mreturn\u001b[39;00m step_output\n",
            "File \u001b[1;32me:\\Applications\\Anaconda\\envs\\swamphacks\\lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:216\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[1;34m(self, optimizer, opt_idx, closure, model, **kwargs)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39m\"\"\"Performs the actual optimizer step.\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \n\u001b[0;32m    208\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[39m    **kwargs: Any extra arguments to ``optimizer.step``\u001b[39;00m\n\u001b[0;32m    214\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    215\u001b[0m model \u001b[39m=\u001b[39m model \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module\n\u001b[1;32m--> 216\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39moptimizer_step(model, optimizer, opt_idx, closure, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32me:\\Applications\\Anaconda\\envs\\swamphacks\\lib\\site-packages\\pytorch_lightning\\plugins\\precision\\precision_plugin.py:153\u001b[0m, in \u001b[0;36mPrecisionPlugin.optimizer_step\u001b[1;34m(self, model, optimizer, optimizer_idx, closure, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(model, pl\u001b[39m.\u001b[39mLightningModule):\n\u001b[0;32m    152\u001b[0m     closure \u001b[39m=\u001b[39m partial(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wrap_closure, model, optimizer, optimizer_idx, closure)\n\u001b[1;32m--> 153\u001b[0m \u001b[39mreturn\u001b[39;00m optimizer\u001b[39m.\u001b[39mstep(closure\u001b[39m=\u001b[39mclosure, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32me:\\Applications\\Anaconda\\envs\\swamphacks\\lib\\site-packages\\torch\\optim\\optimizer.py:113\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m    112\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m--> 113\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32me:\\Applications\\Anaconda\\envs\\swamphacks\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32me:\\Applications\\Anaconda\\envs\\swamphacks\\lib\\site-packages\\torch\\optim\\adamw.py:119\u001b[0m, in \u001b[0;36mAdamW.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[39mif\u001b[39;00m closure \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    118\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39menable_grad():\n\u001b[1;32m--> 119\u001b[0m         loss \u001b[39m=\u001b[39m closure()\n\u001b[0;32m    121\u001b[0m \u001b[39mfor\u001b[39;00m group \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparam_groups:\n\u001b[0;32m    122\u001b[0m     params_with_grad \u001b[39m=\u001b[39m []\n",
            "File \u001b[1;32me:\\Applications\\Anaconda\\envs\\swamphacks\\lib\\site-packages\\pytorch_lightning\\plugins\\precision\\precision_plugin.py:138\u001b[0m, in \u001b[0;36mPrecisionPlugin._wrap_closure\u001b[1;34m(self, model, optimizer, optimizer_idx, closure)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_wrap_closure\u001b[39m(\n\u001b[0;32m    126\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    127\u001b[0m     model: \u001b[39m\"\u001b[39m\u001b[39mpl.LightningModule\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    130\u001b[0m     closure: Callable[[], Any],\n\u001b[0;32m    131\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    132\u001b[0m     \u001b[39m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the\u001b[39;00m\n\u001b[0;32m    133\u001b[0m \u001b[39m    ``on_before_optimizer_step`` hook is called.\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \n\u001b[0;32m    135\u001b[0m \u001b[39m    The closure (generally) runs ``backward`` so this allows inspecting gradients in this hook. This structure is\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[39m    consistent with the ``PrecisionPlugin`` subclasses that cannot pass ``optimizer.step(closure)`` directly.\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 138\u001b[0m     closure_result \u001b[39m=\u001b[39m closure()\n\u001b[0;32m    139\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_after_closure(model, optimizer, optimizer_idx)\n\u001b[0;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m closure_result\n",
            "File \u001b[1;32me:\\Applications\\Anaconda\\envs\\swamphacks\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\optimizer_loop.py:146\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Optional[Tensor]:\n\u001b[1;32m--> 146\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclosure(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    147\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\u001b[39m.\u001b[39mloss\n",
            "File \u001b[1;32me:\\Applications\\Anaconda\\envs\\swamphacks\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\optimizer_loop.py:132\u001b[0m, in \u001b[0;36mClosure.closure\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclosure\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ClosureResult:\n\u001b[1;32m--> 132\u001b[0m     step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_step_fn()\n\u001b[0;32m    134\u001b[0m     \u001b[39mif\u001b[39;00m step_output\u001b[39m.\u001b[39mclosure_loss \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    135\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwarning_cache\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[1;32me:\\Applications\\Anaconda\\envs\\swamphacks\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\optimizer_loop.py:407\u001b[0m, in \u001b[0;36mOptimizerLoop._training_step\u001b[1;34m(self, kwargs)\u001b[0m\n\u001b[0;32m    398\u001b[0m \u001b[39m\"\"\"Performs the actual train step with the tied hooks.\u001b[39;00m\n\u001b[0;32m    399\u001b[0m \n\u001b[0;32m    400\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    404\u001b[0m \u001b[39m    A ``ClosureResult`` containing the training step output.\u001b[39;00m\n\u001b[0;32m    405\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    406\u001b[0m \u001b[39m# manually capture logged metrics\u001b[39;00m\n\u001b[1;32m--> 407\u001b[0m training_step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49m_call_strategy_hook(\u001b[39m\"\u001b[39;49m\u001b[39mtraining_step\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49mkwargs\u001b[39m.\u001b[39;49mvalues())\n\u001b[0;32m    408\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mpost_training_step()\n\u001b[0;32m    410\u001b[0m model_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39m_call_lightning_module_hook(\u001b[39m\"\u001b[39m\u001b[39mtraining_step_end\u001b[39m\u001b[39m\"\u001b[39m, training_step_output)\n",
            "File \u001b[1;32me:\\Applications\\Anaconda\\envs\\swamphacks\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1706\u001b[0m, in \u001b[0;36mTrainer._call_strategy_hook\u001b[1;34m(self, hook_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1703\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m   1705\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[1;32m-> 1706\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1708\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m   1709\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
            "File \u001b[1;32me:\\Applications\\Anaconda\\envs\\swamphacks\\lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:358\u001b[0m, in \u001b[0;36mStrategy.training_step\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    356\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39mtrain_step_context():\n\u001b[0;32m    357\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, TrainingStep)\n\u001b[1;32m--> 358\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtraining_step(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "\u001b[1;32me:\\Projects\\Lab\\MRI\\main.ipynb Cell 13\u001b[0m in \u001b[0;36mSegmenter.training_step\u001b[1;34m(self, train_batch, batch_index)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Projects/Lab/MRI/main.ipynb#X15sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(inputs)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Projects/Lab/MRI/main.ipynb#X15sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     probabilities \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39msoftmax(logits, dim\u001b[39m=\u001b[39mCHANNELS_DIMENSION)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/Projects/Lab/MRI/main.ipynb#X15sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     batch_losses \u001b[39m=\u001b[39m get_dice_loss(probabilities, targets\u001b[39m.\u001b[39;49mshort())\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Projects/Lab/MRI/main.ipynb#X15sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     batch_loss \u001b[39m=\u001b[39m batch_losses\u001b[39m.\u001b[39mmean()\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Projects/Lab/MRI/main.ipynb#X15sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog(\u001b[39m'\u001b[39m\u001b[39mtrain_loss\u001b[39m\u001b[39m'\u001b[39m, batch_loss\u001b[39m.\u001b[39mitem())\n",
            "\u001b[1;32me:\\Projects\\Lab\\MRI\\main.ipynb Cell 13\u001b[0m in \u001b[0;36mget_dice_loss\u001b[1;34m(output, target)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Projects/Lab/MRI/main.ipynb#X15sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_dice_loss\u001b[39m(output, target):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/Projects/Lab/MRI/main.ipynb#X15sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m get_dice_score(output, target)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Projects/Lab/MRI/main.ipynb#X15sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(loss) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Projects/Lab/MRI/main.ipynb#X15sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n",
            "\u001b[1;32me:\\Projects\\Lab\\MRI\\main.ipynb Cell 13\u001b[0m in \u001b[0;36mget_dice_score\u001b[1;34m(output, target, epsilon)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Projects/Lab/MRI/main.ipynb#X15sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m p1 \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m p0\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Projects/Lab/MRI/main.ipynb#X15sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m g1 \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m g0\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/Projects/Lab/MRI/main.ipynb#X15sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m tp \u001b[39m=\u001b[39m (p0 \u001b[39m*\u001b[39;49m g0)\u001b[39m.\u001b[39;49msum(dim\u001b[39m=\u001b[39;49mSPATIAL_DIMENSIONS)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Projects/Lab/MRI/main.ipynb#X15sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m fp \u001b[39m=\u001b[39m (p0 \u001b[39m*\u001b[39m g1)\u001b[39m.\u001b[39msum(dim\u001b[39m=\u001b[39mSPATIAL_DIMENSIONS)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Projects/Lab/MRI/main.ipynb#X15sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m fn \u001b[39m=\u001b[39m (p1 \u001b[39m*\u001b[39m g0)\u001b[39m.\u001b[39msum(dim\u001b[39m=\u001b[39mSPATIAL_DIMENSIONS)\n",
            "\u001b[1;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-5, 4], but got 5)"
          ]
        }
      ],
      "source": [
        "pred = trainer.fit(model, module.train_dataloader(), module.val_dataloader())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "d = module.train_dataloader()\n",
        "d = iter(d)\n",
        "sub =  next(d)\n",
        "inputs = sub['sample'][tio.DATA]\n",
        "# print(inputs)\n",
        "# tensor_to_video(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n = inputs.numpy()\n",
        "n = n[:,:,0,0,0]\n",
        "print(np.shape(n))\n",
        "print(n[:,0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'inputs' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32me:\\Projects\\Lab\\MRI\\main.ipynb Cell 16\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Projects/Lab/MRI/main.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m inputs\u001b[39m.\u001b[39mshape()\n",
            "\u001b[1;31mNameError\u001b[0m: name 'inputs' is not defined"
          ]
        }
      ],
      "source": [
        "inputs.shape()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Unconverted Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_model_and_optimizer(device):\n",
        "    model = UNet(\n",
        "        in_channels=1,\n",
        "        out_classes=6,\n",
        "        dimensions=3,\n",
        "        num_encoding_blocks=wandb.config['num_encoding_blocks'],\n",
        "        out_channels_first_layer=wandb.config['out_channels_first_layer'],\n",
        "        normalization='batch',\n",
        "        upsampling_type='linear',\n",
        "        padding=True,\n",
        "        activation=wandb.config['activation'],\n",
        "    ).to(device)\n",
        "    # print(model.parameters)\n",
        "    \n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=wandb.config['learning_rate'])\n",
        "    return model, optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title (Deep learning functions, double-click here to expand)\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else 'cpu'\n",
        "CHANNELS_DIMENSION = 1\n",
        "SPATIAL_DIMENSIONS = 2, 3, 4\n",
        "\n",
        "class Action(enum.Enum):\n",
        "    TRAIN = 'Training'\n",
        "    VALIDATE = 'Validation'\n",
        "\n",
        "# def prepare_batch(batch, device):\n",
        "#     if device != None:\n",
        "#         inputs = batch['sample'][tio.DATA].to(device)\n",
        "#         targets = batch['label'][tio.DATA].to(device)\n",
        "#     else:\n",
        "#         inputs = batch['sample'][tio.DATA]\n",
        "#         targets = batch['label'][tio.DATA]\n",
        "\n",
        "#     print(f\"DTYPE: {type(inputs)}\")\n",
        "#     return inputs, targets\n",
        "\n",
        "def get_dice_score(output, target, epsilon=1e-9):\n",
        "    p0 = output\n",
        "    g0 = target\n",
        "    p1 = 1 - p0\n",
        "    g1 = 1 - g0\n",
        "    tp = (p0 * g0).sum(dim=SPATIAL_DIMENSIONS)\n",
        "    fp = (p0 * g1).sum(dim=SPATIAL_DIMENSIONS)\n",
        "    fn = (p1 * g0).sum(dim=SPATIAL_DIMENSIONS)\n",
        "    num = 2 * tp\n",
        "    denom = 2 * tp + fp + fn + epsilon\n",
        "    dice_score = num / denom\n",
        "    return dice_score\n",
        "\n",
        "def get_dice_loss(output, target):\n",
        "    return 1 - get_dice_score(output, target)\n",
        "\n",
        "def get_model_and_optimizer(device):\n",
        "    model = UNet(\n",
        "        in_channels=1,\n",
        "        out_classes=6,\n",
        "        dimensions=3,\n",
        "        num_encoding_blocks=wandb.config['num_encoding_blocks'],\n",
        "        out_channels_first_layer=wandb.config['out_channels_first_layer'],\n",
        "        normalization='batch',\n",
        "        upsampling_type='linear',\n",
        "        padding=True,\n",
        "        activation=wandb.config['activation'],\n",
        "    ).to(device)\n",
        "    # print(model.parameters)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=wandb.config['learning_rate'])\n",
        "    return model, optimizer\n",
        "\n",
        "def run_epoch(epoch_idx, action, loader, model, optimizer):\n",
        "    is_training = action == Action.TRAIN\n",
        "    epoch_losses = []\n",
        "    times = []\n",
        "    model.train(is_training)\n",
        "    for batch_idx, batch in enumerate(tqdm(loader)):\n",
        "        inputs, targets = prepare_batch(batch, device)\n",
        "        optimizer.zero_grad()\n",
        "        with torch.set_grad_enabled(is_training):\n",
        "            logits = model(inputs)\n",
        "            print(f\"dtype: {inputs[0][0][0]}\")\n",
        "            probabilities = F.softmax(logits, dim=CHANNELS_DIMENSION)\n",
        "            batch_losses = get_dice_loss(probabilities, targets.short())\n",
        "            batch_loss = batch_losses.mean()\n",
        "            if is_training:\n",
        "                batch_loss.backward()\n",
        "                optimizer.step()\n",
        "            times.append(time.time())\n",
        "            epoch_losses.append(batch_loss.item())\n",
        "    epoch_losses = np.array(epoch_losses)\n",
        "    print(f'{action.value} mean loss: {epoch_losses.mean()}')\n",
        "    return times, epoch_losses\n",
        "\n",
        "def train(num_epochs, training_loader, validation_loader, model, optimizer, weights_stem):\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "   \n",
        "\n",
        "    # val_losses.append(run_epoch(0, Action.VALIDATE, validation_loader, model, optimizer))\n",
        "    for epoch_idx in range(1, num_epochs + 1):\n",
        "\n",
        "\n",
        "        print('Starting epoch', epoch_idx)\n",
        "        train_loss = run_epoch(epoch_idx, Action.TRAIN, training_loader, model, optimizer)\n",
        "        train_losses.append(train_loss)\n",
        "        val_loss = run_epoch(epoch_idx, Action.VALIDATE, validation_loader, model, optimizer)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        wandb.log({\"val_loss\": val_loss})\n",
        "        wandb.log({\"train_loss\": train_loss})\n",
        "\n",
        "        torch.save(model.state_dict(), f'{weights_stem}_epoch_{epoch_idx}.pth')\n",
        "    return np.array(train_losses), np.array(val_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "module.training_subjects[1].plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tens = training_subjects[0].sample.data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "ind = random.randint(0,tens.size()[1])\n",
        "slice = tens[:, ind,:,:]\n",
        "import torchvision.transforms.functional as Ft\n",
        "img = Ft.to_pil_image(slice)\n",
        "\n",
        "display(img)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "edited_tens = "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#plot 1 tensor, verify color space redux works"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dic = {}\n",
        "dic['sample'] = it['sample'].data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWbWAyFNTj74"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import wandb\n",
        "wandb.init(project=\"3dUnet\")\n",
        "wandb.config = {\n",
        "    #Model Parameters\n",
        "  \"num_encoding_blocks\":3,\n",
        "  \"out_channels_first_layer\":8,\n",
        "  \"activation\": \"PReLU\",\n",
        "    #Training Parameters\n",
        "  \"learning_rate\": 0.001,\n",
        "  \"epochs\": 10,\n",
        "  \"train_batch_size\": 4,\n",
        "  \"val_batch_size\":4\n",
        "  \n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if (torch.cuda.is_available()):\n",
        "    print(\"Cuda is available\")\n",
        "model, optimizer = get_model_and_optimizer(device)\n",
        "# wandb.watch(model)\n",
        "weights_path = 'whole_image_state_dict.pth'\n",
        "training_loader = torch.utils.data.DataLoader(\n",
        "    training_set,\n",
        "    batch_size=wandb.config['train_batch_size'],\n",
        "    shuffle=True,\n",
        "    num_workers=1,\n",
        ")\n",
        "\n",
        "validation_loader = torch.utils.data.DataLoader(\n",
        "    validation_set,\n",
        "    batch_size=wandb.config['val_batch_size'],\n",
        "    num_workers=1,\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "LFpQsUAyPxeA",
        "outputId": "4c4eef66-1ef6-423e-ee9d-63bc583f40c1"
      },
      "outputs": [],
      "source": [
        "weights_stem = 'whole_images'\n",
        "train_losses, val_losses = train(wandb.config['epochs'], training_loader, validation_loader, model, optimizer, weights_stem)\n",
        "checkpoint = {\n",
        "    'train_losses': train_losses,\n",
        "    'val_losses': val_losses,\n",
        "    'weights': model.state_dict(),\n",
        "}\n",
        "torch.save(checkpoint, weights_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sS4xOEbwhKVU"
      },
      "source": [
        "#### Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def staty(image):\n",
        "    print(f\"This image is {image.size[0]} by {image.size[1]}\")\n",
        "    arr = np.array(image)\n",
        "    print(f\"Mean pixel value is {np.mean(arr)}\")\n",
        "    print(f\"Median pixel value is {np.median(arr)}\")\n",
        "    print(f\"Minimum is {np.min(arr)}, Max is {np.max(arr)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torchvision.transforms as tv\n",
        "convert = tv.Compose([tv.ToPILImage()])\n",
        "slices = []\n",
        "for i in range(6):\n",
        "    slices.append(probabilities[:,i,24,:,:])\n",
        "\n",
        "# im = convert(slices[1])\n",
        "# imshow(im)\n",
        "# stats(im)\n",
        "affine = batch['sample'][tio.AFFINE][0].numpy()\n",
        "ind = 24\n",
        "\n",
        "pl1 = inputs.cpu()\n",
        "pl2 = targets.cpu()\n",
        "probs = probabilities.cpu()\n",
        "subject = tio.Subject(\n",
        "    scan=tio.ScalarImage(tensor=pl[:,:,ind,:,:]),\n",
        "    label1=tio.LabelMap(tensor=pl2[:,:,ind,:,:],),\n",
        "    predicted=tio.ScalarImage(tensor=probs[:,:,ind,:,:]),\n",
        ")\n",
        "subject.plot(figsize=(9, 8))#, cmap_dict={'predicted': 'RdBu_r'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "id": "U-5ZGq5UZ5QK",
        "outputId": "28d5b7d9-7e1e-46dd-c71f-df30880fbd87"
      },
      "outputs": [],
      "source": [
        "# batch = next(iter(validation_loader))\n",
        "batch = next(iter(training_loader))\n",
        "\n",
        "model.eval()\n",
        "inputs, targets = prepare_batch(batch, device)\n",
        "FIRST = 0\n",
        "FOREGROUND = 1\n",
        "with torch.no_grad():\n",
        "    probabilities = model(inputs).softmax(dim=1)[:, FOREGROUND:].cpu()\n",
        "affine = batch['sample'][tio.AFFINE][0].numpy()\n",
        "subject = tio.Subject(\n",
        "    mri=tio.ScalarImage(tensor=batch['sample'][tio.DATA][FIRST], affine=affine),\n",
        "    label=tio.LabelMap(tensor=batch['label'][tio.DATA][FIRST], affine=affine),\n",
        "    predicted=tio.ScalarImage(tensor=probabilities[FIRST], affine=affine),\n",
        ")\n",
        "subject.plot(figsize=(9, 8), cmap_dict={'predicted': 'RdBu_r'})"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "TorchIO tutorial.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.10 ('swamphacks')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "6a4907715359045b8a2ace855fc06a352fe7b25f30531dd2e3f9c93331a0a772"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
