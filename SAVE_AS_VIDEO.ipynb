{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 512, 75])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import PIL\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "samp = np.load(os.getcwd() + \"/DATA/sample/sample0.npy\")\n",
    "label = np.load(os.getcwd() + \"/DATA/label/label0.npy\")\n",
    "SLICE_START = 0\n",
    "SLICE_LENGTH = 5\n",
    "sample = torch.tensor(samp)\n",
    "print(sample.shape)\n",
    "label = torch.tensor(label)\n",
    "sample = torch.narrow(sample, 3, SLICE_START, SLICE_LENGTH)\n",
    "label = torch.narrow(label, 3, SLICE_START, SLICE_LENGTH)\n",
    "lab0 = torch.where(label==0,1,0);lab1 = torch.where(label==1,1,0)\n",
    "lab2 = torch.where(label==2,1,0);lab3 = torch.where(label==3,1,0)\n",
    "lab4 = torch.where(label==4,1,0);lab5 = torch.where(label==5,1,0)\n",
    "master_label = torch.stack([lab0,lab1,lab2,lab3,lab4,lab5], dim = 1)\n",
    "\n",
    "\n",
    "\n",
    "from matplotlib import cm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = torch.reshape(sample, ( 1,1,sample.shape[3], sample.shape[1], sample.shape[2]))\n",
    "master_label = torch.reshape(master_label, (1, master_label.shape[1],master_label.shape[4], master_label.shape[2], master_label.shape[3]))\n",
    "print(master_label.shape[1])\n",
    "\n",
    "print( sample.shape, master_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def label_to_video(inputs, name=\"label_vid\", repeat=3):\n",
    "    \"\"\"\n",
    "    Given an output tensor from the network with shape [6, 256, 256, 10]), return a\n",
    "    video of the tensor with name \"name\"\n",
    "    \"\"\"\n",
    "    num_classes=6\n",
    "    color = (torch.ones((1,inputs.shape[1],inputs.shape[2],inputs.shape[3]))/num_classes)\n",
    "    re_colored = inputs * color\n",
    "    re_colored = re_colored[0]\n",
    "    slices = []\n",
    "    magma = cm.get_cmap('YlGnBu')\n",
    "    for i in range(re_colored.size()[2]):\n",
    "        new = re_colored[:,:,i]\n",
    "        new = magma(new)\n",
    "        new = new[:,:,:3]*255\n",
    "        pil_image = PIL.Image.fromarray(new.astype(np.uint8))\n",
    "#         display(pil_image)\n",
    "        \n",
    "    # return slices\n",
    "\n",
    "\n",
    "\n",
    "#sample\n",
    "img_size = (SLICE_LENGTH, 256, 256)\n",
    "\n",
    "\n",
    "# sample = torch.ones(1, 1, img_size[0], img_size[1], img_size[2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e:\\Applications\\Anaconda\\envs\\new\\lib\\site-packages\\vit_pytorch\\__init__.py\n"
     ]
    }
   ],
   "source": [
    "import vit_pytorch\n",
    "print(vit_pytorch.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "#expects tensor of shape (1,1,slices,W,H)\n",
    "def sample_to_slices(inputs, name=\"label_vid\", repeat=3):\n",
    "    inputs = inputs[0]\n",
    "    slices = []\n",
    "    print(f\"sampshape: {inputs.shape}\")\n",
    "    for i in range(inputs.size()[2]):\n",
    "        new = inputs[:,:,i]\n",
    "        pil_image = transform(new)\n",
    "        # display(pil_image)\n",
    "        for _ in range(repeat):\n",
    "            slices.append(pil_image)\n",
    "        \n",
    "    return slices\n",
    "\n",
    "def write_video(file_path, frames, fps, grayscale=False):\n",
    "    \"\"\"\n",
    "    Writes frames to an mp4 video file\n",
    "    :param file_path: Path to output video, must end with .mp4\n",
    "    :param frames: List of PIL.Image objects\n",
    "    :param fps: Desired frame rate\n",
    "    \"\"\"\n",
    "    w, h = frames[0].size\n",
    "    fourcc = cv2.VideoWriter_fourcc('m', 'p', '4', 'v')\n",
    "    writer = cv2.VideoWriter(file_path, fourcc, fps, (w, h))\n",
    "\n",
    "    for frame in frames:\n",
    "        # open_cv_image = np.asarray(frame)\n",
    "        # # print(f\"frameshape: {open_cv_image.shape}\")\n",
    "        # # img = Image.fromarray(open_cv_image)\n",
    "        # # img.save('nu.png')\n",
    "        # # break\n",
    "        # if not grayscale:\n",
    "        #     # Convert RGB to BGR \n",
    "        #     open_cv_image = open_cv_image[:, :, ::-1]\n",
    "           \n",
    "        # else:\n",
    "        #     open_cv_image = cv2.cvtColor(open_cv_image, cv2.COLOR_GRAY2BGR)\n",
    "        open_cv_image = cv2.cvtColor(np.array(frame), cv2.COLOR_RGB2BGR)\n",
    "        writer.write(open_cv_image)\n",
    "\n",
    "    writer.release() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAMPLE TO VIDEO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "transform = T.ToPILImage()\n",
    "\n",
    "#expects tensor of shape (1,1,slices,W,H)\n",
    "def sample_to_slices(inputs, name=\"label_vid\", repeat=3):\n",
    "    inputs = inputs[0]\n",
    "    slices = []\n",
    "    print(f\"sampshape: {inputs.shape}\")\n",
    "    for i in range(inputs.size()[2]):\n",
    "        new = inputs[:,:,i]\n",
    "        pil_image = transform(new)\n",
    "        for _ in range(repeat):\n",
    "            slices.append(pil_image)\n",
    "        \n",
    "    return slices\n",
    "\n",
    "def write_video(file_path, frames, fps):\n",
    "    \"\"\"\n",
    "    Writes frames to an mp4 video file\n",
    "    :param file_path: Path to output video, must end with .mp4\n",
    "    :param frames: List of PIL.Image objects\n",
    "    :param fps: Desired frame rate\n",
    "    \"\"\"\n",
    "    w, h = frames[0].size\n",
    "    fourcc = cv2.VideoWriter_fourcc('m', 'p', '4', 'v')\n",
    "    writer = cv2.VideoWriter(file_path, fourcc, fps, (w, h))\n",
    "\n",
    "    for frame in frames:\n",
    "        open_cv_image = cv2.cvtColor(np.array(frame), cv2.COLOR_RGB2BGR)\n",
    "        writer.write(open_cv_image)\n",
    "\n",
    "    writer.release() \n",
    "\n",
    "\n",
    "def sample_to_video(sample, filename, repeat, fps):\n",
    "    slices=sample_to_slices(sample, repeat)\n",
    "    write_video(os.getcwd() + f\"/{filename}.mp4\", slices, fps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 512, 5])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_to_video(sample, \"new\", repeat=1, fps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Not compiled with video_reader support, to enable video_reader support, please install ffmpeg (version 4.2 is currently supported) and build torchvision from source.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32me:\\Projects\\Lab\\MRI\\SAVE_AS_VIDEO.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Projects/Lab/MRI/SAVE_AS_VIDEO.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Projects/Lab/MRI/SAVE_AS_VIDEO.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m video_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnew.mp4\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Projects/Lab/MRI/SAVE_AS_VIDEO.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m reader \u001b[39m=\u001b[39m torchvision\u001b[39m.\u001b[39;49mio\u001b[39m.\u001b[39;49mVideoReader(video_path, \u001b[39m\"\u001b[39;49m\u001b[39mvideo\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\io\\video_reader.py:101\u001b[0m, in \u001b[0;36mVideoReader.__init__\u001b[1;34m(self, path, stream, num_threads, device)\u001b[0m\n\u001b[0;32m     99\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _has_video_opt():\n\u001b[1;32m--> 101\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    102\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNot compiled with video_reader support, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    103\u001b[0m         \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mto enable video_reader support, please install \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    104\u001b[0m         \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mffmpeg (version 4.2 is currently supported) and \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    105\u001b[0m         \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbuild torchvision from source.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    106\u001b[0m     )\n\u001b[0;32m    108\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_c \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mclasses\u001b[39m.\u001b[39mtorchvision\u001b[39m.\u001b[39mVideo(path, stream, num_threads)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Not compiled with video_reader support, to enable video_reader support, please install ffmpeg (version 4.2 is currently supported) and build torchvision from source."
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "video_path = \"new.mp4\"\n",
    "reader = torchvision.io.VideoReader(video_path, \"video\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip inst"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit ('new')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5c39067fb85dad44b96a991ff273746e02ffde2a07ab0e300120c2298a074d00"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
